import requests
import xml.etree.ElementTree as ET
import pandas as pd
import streamlit as st
from datetime import datetime
from dateutil.relativedelta import relativedelta
import time
import os

# ==========================================
# [ì„¤ì •] API í‚¤ ë° ê²½ë¡œ
# ==========================================
MOLIT_API_KEY = "fba6973ac6f9aed36f2b30b7dcce1fa4f6bef6c6c26cb61aff47144cc68520e5"
KAKAO_API_KEY = "5b71324d3e681cdeaa038e7725055998"

DATA_DIR = r"./data"
if not os.path.exists(DATA_DIR): os.makedirs(DATA_DIR)

# [ìˆ˜ì •] íŒŒì¼ ê²½ë¡œ ì •ë¦¬ (3ê°œë¡œ í†µí•©)
ROOM_CSV_PATH = os.path.join(DATA_DIR, "room.csv")
CCTV_CSV_PATH = os.path.join(DATA_DIR, "cctv.csv")
NOISE_CSV_PATH = os.path.join(DATA_DIR, "noise.csv")          # ìˆ ì§‘ + ë…¸ëž˜ë°©
CONVENIENCE_CSV_PATH = os.path.join(DATA_DIR, "convenience.csv") # íŽ¸ì˜ì 
STORE_CSV_PATH = os.path.join(DATA_DIR, "store.csv")           # ìŒì‹ì  + ì¹´íŽ˜

TARGET_DONGS = ["ì¡°ì˜ë™", "ëŒ€ë™", "ìž„ë‹¹ë™", "ë¶€ì ë¦¬"]

# ==========================================
# 1. êµ­í† ë¶€ ì‹¤ê±°ëž˜ê°€ / CCTV (ê¸°ì¡´ ìœ ì§€)
# ==========================================
def fetch_one_month_data(lawd_cd, deal_ymd):
    url = "http://apis.data.go.kr/1613000/RTMSDataSvcSHRent/getRTMSDataSvcSHRent"
    params = {"serviceKey": MOLIT_API_KEY, "LAWD_CD": lawd_cd, "DEAL_YMD": deal_ymd, "numOfRows": 1000, "pageNo": 1}
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code != 200: return []
        root = ET.fromstring(response.content)
        if root.findtext(".//resultCode") != "000": return []
        data_list = []
        for item in root.findall(".//item"):
            if item.findtext("jibun", "").strip().startswith("ì‚°"): continue
            data_list.append({
                "ë²•ì •ë™": item.findtext("umdNm", "").strip(),
                "ê±´ì¶•ë…„ë„": int(item.findtext("buildYear", "0").strip() or 0),
                "ì „ìš©ë©´ì ": float(item.findtext("totalFloorAr", 0)),
                "ë³´ì¦ê¸ˆ": int(item.findtext("deposit", "0").replace(",", "")),
                "ì›”ì„¸": int(item.findtext("monthlyRent", "0").replace(",", "")),
                "ê³„ì•½ì¼": f"{item.findtext('dealYear')}-{item.findtext('dealMonth')}-{item.findtext('dealDay')}"
            })
        return data_list
    except: return []

def get_real_estate_data(lawd_cd="47290", months=24, force_update=False):
    if os.path.exists(ROOM_CSV_PATH) and not force_update:
        try: df = pd.read_csv(ROOM_CSV_PATH, encoding='utf-8-sig')
        except: df = pd.read_csv(ROOM_CSV_PATH, encoding='cp949')
        df.columns = df.columns.str.replace('\ufeff', '').str.strip()
        if 'ë²•ì •ë™' in df.columns:
            mask = df['ë²•ì •ë™'].apply(lambda x: any(target in str(x) for target in TARGET_DONGS))
            return df[mask]
    
    date_list = [ (datetime.now() - relativedelta(months=i)).strftime("%Y%m") for i in range(months) ]
    all_data = []
    for ymd in date_list:
        all_data.extend(fetch_one_month_data(lawd_cd, ymd))
        time.sleep(0.05)
    
    if not all_data: return pd.DataFrame()
    df = pd.DataFrame(all_data)
    df.columns = df.columns.str.replace('\ufeff', '').str.strip()
    mask = df['ë²•ì •ë™'].apply(lambda x: any(target in str(x) for target in TARGET_DONGS))
    df_filtered = df[mask].copy()
    current_year = datetime.now().year
    df_filtered['ë…¸í›„ë„'] = df_filtered['ê±´ì¶•ë…„ë„'].apply(lambda x: current_year - x if x > 0 else 0)
    df_filtered.to_csv(ROOM_CSV_PATH, index=False, encoding='utf-8-sig')
    return df_filtered

def get_cctv_data():
    if not os.path.exists(CCTV_CSV_PATH): return pd.DataFrame()
    try:
        try: df = pd.read_csv(CCTV_CSV_PATH, encoding='utf-8-sig')
        except: df = pd.read_csv(CCTV_CSV_PATH, encoding='cp949')
        df.columns = df.columns.str.replace('\ufeff', '').str.strip()
        df = df.rename(columns={'WGS84ìœ„ë„': 'lat', 'WGS84ê²½ë„': 'lon', 'ìœ„ë„': 'lat', 'ê²½ë„': 'lon'})
        if 'lat' in df.columns and 'lon' in df.columns:
            df['lat'] = pd.to_numeric(df['lat'], errors='coerce')
            df['lon'] = pd.to_numeric(df['lon'], errors='coerce')
            return df.dropna(subset=['lat', 'lon']).drop_duplicates(subset=['lat', 'lon'])
    except: pass
    return pd.DataFrame()

# ==========================================
# [ìˆ˜ì •] ì¹´ì¹´ì˜¤ API ë‚´ë¶€ í˜¸ì¶œ í•¨ìˆ˜ (ì €ìž¥ ê¸°ëŠ¥ ì œê±°, ë°ì´í„° ë¦¬í„´ë§Œ)
# ==========================================
def _fetch_api(category_code=None, keyword=None, rect_str=None):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json" if keyword else "https://dapi.kakao.com/v2/local/search/category.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"rect": rect_str, "page": 1, "size": 15}
    if category_code: params["category_group_code"] = category_code
    if keyword: params["query"] = keyword

    results = []
    while True:
        try:
            resp = requests.get(url, headers=headers, params=params, timeout=3)
            if resp.status_code != 200: break
            data = resp.json()
            for doc in data.get('documents', []):
                results.append({
                    "name": doc['place_name'],
                    "lat": float(doc['y']),
                    "lon": float(doc['x']),
                    "category": doc.get('category_name', ''),
                    "url": doc['place_url']
                })
            if not data.get('meta', {}).get('is_end'):
                params['page'] += 1
                if params['page'] > 3: break
            else: break
        except: break
        time.sleep(0.1)
    return pd.DataFrame(results)

# ==========================================
# [í•µì‹¬] 3. ê·¸ë£¹ë³„ ë°ì´í„° ìˆ˜ì§‘ ë° ë³‘í•© ì €ìž¥
# ==========================================
def _get_grouped_data(save_path, fetch_funcs, min_lat, max_lat, min_lon, max_lon, force_update=False):
    # 1. íŒŒì¼ ìžˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(save_path) and not force_update:
        try:
            df = pd.read_csv(save_path, encoding='utf-8-sig')
            # í˜„ìž¬ í™”ë©´ ë²”ìœ„ í•„í„°ë§
            mask = (df['lat'] >= min_lat) & (df['lat'] <= max_lat) & \
                   (df['lon'] >= min_lon) & (df['lon'] <= max_lon)
            if len(df[mask]) > 0: return df[mask]
        except: pass

    # 2. API í˜¸ì¶œ ë° ë³‘í•©
    pad = 0.01
    rect_str = f"{min_lon-pad},{min_lat-pad},{max_lon+pad},{max_lat+pad}"
    
    dfs = []
    print(f"ðŸ“¡ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({os.path.basename(save_path)})")
    
    for func_args, type_name in fetch_funcs:
        # func_args: {'keyword': '...'} or {'category_code': '...'}
        df = _fetch_api(rect_str=rect_str, **func_args)
        if not df.empty:
            df['type'] = type_name # êµ¬ë¶„ê°’ ì¶”ê°€ (ì˜ˆ: ìˆ ì§‘, ë…¸ëž˜ë°©)
            dfs.append(df)
            
    if dfs:
        merged_df = pd.concat(dfs).drop_duplicates(subset=['lat', 'lon'])
        merged_df.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ì €ìž¥ ì™„ë£Œ: {save_path} ({len(merged_df)}ê°œ)")
        
        # ë²”ìœ„ í•„í„°ë§ ë°˜í™˜
        mask = (merged_df['lat'] >= min_lat) & (merged_df['lat'] <= max_lat) & \
               (merged_df['lon'] >= min_lon) & (merged_df['lon'] <= max_lon)
        return merged_df[mask]
    
    return pd.DataFrame()

# --- ì™¸ë¶€ í˜¸ì¶œ í•¨ìˆ˜ë“¤ ---

def get_noise_data(min_lat, max_lat, min_lon, max_lon):
    """ìˆ ì§‘ + ë…¸ëž˜ë°© -> noise.csv"""
    tasks = [
        ({'keyword': 'ìˆ ì§‘'}, 'ìˆ ì§‘'),
        ({'keyword': 'ë…¸ëž˜ë°©'}, 'ë…¸ëž˜ë°©')
    ]
    return _get_grouped_data(NOISE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)

def get_convenience_data(min_lat, max_lat, min_lon, max_lon):
    """íŽ¸ì˜ì  -> convenience.csv"""
    tasks = [
        ({'category_code': 'CS2'}, 'íŽ¸ì˜ì ')
    ]
    return _get_grouped_data(CONVENIENCE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)

def get_store_data(min_lat, max_lat, min_lon, max_lon):
    """ìŒì‹ì  + ì¹´íŽ˜ -> store.csv (ìƒê°€ 1ì¸µ ì¶”ì •ìš©)"""
    tasks = [
        ({'category_code': 'FD6'}, 'ìŒì‹ì '),
        ({'category_code': 'CE7'}, 'ì¹´íŽ˜')
    ]
    return _get_grouped_data(STORE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)

@st.cache_data
def get_lamp_data(
    csv_path: str = "./data/lampost_v2.csv",
    min_lat: float = None, max_lat: float = None,
    min_lon: float = None, max_lon: float = None
):
    """
    ê°€ë¡œë“±/ë³´ì•ˆë“± CSV ë¡œë“œ + lat/lon ì •ê·œí™”
    - ì»¬ëŸ¼ëª…ì´ (ìœ„ë„, ê²½ë„) ë˜ëŠ” (lat, lon) ë˜ëŠ” ìœ ì‚¬ ì´ë¦„ì´ì–´ë„ ìµœëŒ€í•œ ìžë™ ì¸ì‹
    - ì¸ì½”ë”© utf-8 / cp949 ìžë™ ì²˜ë¦¬
    - boundsê°€ ì£¼ì–´ì§€ë©´ ì¦‰ì‹œ ë²”ìœ„ í•„í„°ë§
    """
    if not os.path.exists(csv_path):
        # íŒŒì¼ëª…ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìžˆì–´ì„œ í”í•œ í›„ë³´ë¥¼ ìžë™ íƒìƒ‰
        candidates = [
            "./data/lampost.csv",
            "./data/lamps.csv",
            "./data/lamp.csv",
            "./data/ê²½ìƒë¶ë„_ê²½ì‚°ì‹œ_ë³´ì•ˆë“±ì •ë³´_20250731.csv",
            "./data/ê²½ìƒë¶ë„_ê²½ì‚°ì‹œ_ì˜ë‚¨ëŒ€ì—­_ë³´ì•ˆë“±.csv",
            "./data/ê²½ìƒë¶ë„_ê²½ì‚°ì‹œ_ì˜ë‚¨ëŒ€ì—­_ë³´ì•ˆë™.csv",  # í˜¹ì‹œ ì˜¤íƒ€ íŒŒì¼ë„ ëŒ€ë¹„
        ]
        found = None
        for c in candidates:
            if os.path.exists(c):
                found = c
                break
        if found is None:
            # Home.pyì—ì„œ st.error ì²˜ë¦¬í•˜ëŠ” íŽ¸ì´ ìžì—°ìŠ¤ëŸ½ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë¹ˆ DF ë°˜í™˜
            return pd.DataFrame(columns=["lat", "lon"])
        csv_path = found

    # ì¸ì½”ë”© ìžë™
    try:
        df = pd.read_csv(csv_path, encoding="utf-8", low_memory=False)
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding="cp949", low_memory=False)

    # 1) ìœ„ë„/ê²½ë„ ì»¬ëŸ¼ ìžë™ íƒìƒ‰
    cols = [c.strip() for c in df.columns]
    df.columns = cols

    # í›„ë³´ ì´ë¦„ë“¤
    lat_candidates = ["lat", "latitude", "ìœ„ë„", "LAT", "Latitude", "ìœ„ë„ê°’"]
    lon_candidates = ["lon", "lng", "long", "longitude", "ê²½ë„", "LON", "Longitude", "ê²½ë„ê°’"]

    lat_col = next((c for c in df.columns if c in lat_candidates), None)
    lon_col = next((c for c in df.columns if c in lon_candidates), None)

    # 2) ê·¸ëž˜ë„ ëª» ì°¾ìœ¼ë©´ â€œë¶€ë¶„ ë¬¸ìžì—´â€ë¡œ ìµœëŒ€í•œ ì°¾ê¸°
    if lat_col is None:
        lat_col = next((c for c in df.columns if "ìœ„ë„" in c or "lat" in c.lower()), None)
    if lon_col is None:
        lon_col = next((c for c in df.columns if "ê²½ë„" in c or "lon" in c.lower() or "lng" in c.lower()), None)

    if lat_col is None or lon_col is None:
        # ì»¬ëŸ¼ì„ ëª» ì°¾ìœ¼ë©´ ë¹ˆ DF
        return pd.DataFrame(columns=["lat", "lon"])

    # 3) ìˆ«ìž ë³€í™˜ + ê²°ì¸¡ ì œê±°
    df["lat"] = pd.to_numeric(df[lat_col], errors="coerce")
    df["lon"] = pd.to_numeric(df[lon_col], errors="coerce")
    df = df.dropna(subset=["lat", "lon"]).copy()

    # 4) bounds í•„í„°
    if None not in (min_lat, max_lat, min_lon, max_lon):
        df = df[
            (df["lat"] >= min_lat) & (df["lat"] <= max_lat) &
            (df["lon"] >= min_lon) & (df["lon"] <= max_lon)
        ].copy()

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€(ì›í•˜ë©´ ì›ë³¸ ì»¬ëŸ¼ë„ ìœ ì§€ ê°€ëŠ¥)
    return df[["lat", "lon"]].reset_index(drop=True)